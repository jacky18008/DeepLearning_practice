{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A CNN model of discrimination using CIFAR-10, I made a reference to http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/ of the model, and changed some parameter such as activate function, dropout rate and learning rate after checking VGG16, and http://localhost:8888/notebooks/GAN_on_Keras_by_osh.ipynb, a GAN model's discriminative part done by Tim O'Shea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN model for CIFAR-10\n",
    "%matplotlib inline\n",
    "import numpy\n",
    "import os,random\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 3, 32, 32)     9248        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 3, 32, 32)     0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 3, 32, 32)     9248        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_1 (LeakyReLU)          (None, 3, 32, 32)     0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 1, 16, 32)     0           leakyrelu_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 512)           0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           262656      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            5130        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 286282\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(3, 32, 32), border_mode='same', activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(32, 3, 3, activation='linear', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model.add(LeakyReLU(alpha=0.2)) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 176s - loss: 1.8410 - acc: 0.3311 - val_loss: 1.5414 - val_acc: 0.4416\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 161s - loss: 1.5581 - acc: 0.4410 - val_loss: 1.4194 - val_acc: 0.4887\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 458s - loss: 1.4544 - acc: 0.4777 - val_loss: 1.3826 - val_acc: 0.5043\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 179s - loss: 1.3368 - acc: 0.5215 - val_loss: 1.2703 - val_acc: 0.5438\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 179s - loss: 1.3003 - acc: 0.5324 - val_loss: 1.2489 - val_acc: 0.5537\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 583s - loss: 1.2125 - acc: 0.5667 - val_loss: 1.1831 - val_acc: 0.5771\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 489s - loss: 1.2006 - acc: 0.5730 - val_loss: 1.1604 - val_acc: 0.5871\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 83s - loss: 1.1837 - acc: 0.5754 - val_loss: 1.1519 - val_acc: 0.5873\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 77s - loss: 1.1758 - acc: 0.5787 - val_loss: 1.1517 - val_acc: 0.5894\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 76s - loss: 1.1634 - acc: 0.5835 - val_loss: 1.1513 - val_acc: 0.5918\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 88s - loss: 1.1520 - acc: 0.5885 - val_loss: 1.1388 - val_acc: 0.5930\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 80s - loss: 1.1419 - acc: 0.5927 - val_loss: 1.1470 - val_acc: 0.5901\n",
      "Epoch 17/25\n",
      "30240/50000 [=================>............] - ETA: 31s - loss: 1.0820 - acc: 0.6142"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=epochs, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
